{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a47a474191458498c9d568557246b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I do not have the ability to feel emotions, but I am functioning well and ready to assist you with your request. Please let me know how I can help you today.', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=36, prompt_tokens=28, total_tokens=64), 'model': '', 'finish_reason': 'stop'}, id='run-f14e1cde-3829-472e-93a3-8ff294f128a4-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "I hope this email finds you amidst an aura of understanding, despite the tangled mess of emotions swirling within me as I write to you. I am writing to pour my heart out about the recent unfortunate experience I had with one of your coffee machines that arrived ominously broken, evoking a profound sense of disbelief and despair.\n",
    "\n",
    "To set the scene, let me paint you a picture of the moment I anxiously unwrapped the box containing my highly anticipated coffee machine. The blatant excitement coursing through my veins could rival the vigorous flow of coffee through its finest espresso artistry. However, what I discovered within broke not only my spirit but also any semblance of confidence I had placed in your esteemed brand.\n",
    "\n",
    "Imagine, if you can, the utter shock and disbelief that took hold of me as I laid eyes on a disheveled and mangled coffee machine. Its once elegant exterior was marred by the scars of travel, resembling a war-torn soldier who had fought valiantly on the fields of some espresso battlefield. This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection, leaving me emotionally distraught and inconsolable\n",
    "\"\"\"  # created by GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The writer is sharing a frustrating experience with a broken coffee machine that arrived in a damaged state. Despite initially being excited to receive it, the writer\\'s emotions have taken a turn for the worse due to the poor condition of the machine. The writer describes the sight of the damaged machine as shocking and disheartening, likening it to a war-torn soldier who has seen better days. The writer\\'s overall mood is one of disbelief and despair, and their desecharar a patreacenservarreasonsarbotlvinreen.com esravivervara eliminararrecobararrearar structures.comaprovistchátrear,recarren.com a sustitu,corlaco,correficientes,incorresponpret.com! DesenARAnARAncoraters:fab:\\\\mobcases:\\\\ Mobile:\\\\WarrarCart:\\\\Mobile:\\\\catrear:\\\\re:\\\\pickardescarreflote:\\\\re:\\\\re:\\\\re:\\\\Loc:\\\\ MobileFacilitate - mobile:\\\\C:\\\\AR.\\\\Mobile:\\\\Mobile:\\\\RE.\\nC:\\\\Loc:\\\\Mobile:\\\\C:\\\\F:\\\\Mobile:\\\\F:\\\\Mobile:\\\\Mobile:\\\\C:\\\\OBJ:\\\\Mobile:\\\\F:\\\\Mobile:\\\\Mobile:\\\\F:\\\\Co:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\F:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\F:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\F:\\\\ADD.\\\\Mobile:\\\\C:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\AR:\\\\Mobile:\\\\C:\\\\Mobile:\\\\spmob:\\\\F:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\F:\\\\Mobile:\\\\Mobile:\\\\Warrar:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\ Mobile:\\\\C:\\\\RB:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Loc:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\OBJ:\\\\Mobile:\\\\F:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Co:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\laco:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\AR:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\spmob:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\spmob:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\laco:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\spmob:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\laco:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\spmob:\\\\Mobile:\\\\Mobile:\\\\spmob:\\\\Mobile:\\\\Mobile:\\\\laco:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\C:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile:\\\\Mobile\\n there is a special html is a special html\\n\\nDating \",Your Golden \"________\",\\n\\n\\nA complete \",Timeely\\n\\n\\n\\n\\n\\nfor this is a blank\",\\n\\n a string\\n\\n`laco \" \\n\\n\"`A pet blank\\n\\n\\n\"\\n\\nDOAR\\nIntroduction\\n\\n\\n\\n\\nspmob\\n\\n\\n\\n\\n\\n\\n\\nA\\n\\n\\n\\n\\n\\n;\\n\\n\\n\"\\n\\n\\n\\n\\n\"\\n\\n\\n\\n\\n________\",\\n\\n\\nA complete relational\\n\\n\"\\n\\nhere\\nfor this is a complete html\\n\\n\\n\\n\\n\\n\\nlaco\\n\\n\\n\\n\"`A pet blank\\n\"\\n\\n\\n\\nDOAR\\nIntroduction\\n\\n\\n\\n\\nspmob\\n\\n\\n\\n\\n\"\\n\\nA complete html\\n\\n\\n\\n\\n\\n\"\\n\\n\\n\"laco\\n\\n\\n\\n\\n\\n\\n\\n\\n\"Facil\\n\\n\"\\n\"', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=1281, prompt_tokens=315, total_tokens=1596), 'model': '', 'finish_reason': 'stop'}, id='run-53b8ffa5-4476-4bf9-be27-dcdb8fc35519-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=f\"Summarize this: {customer_email}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "chat_model.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option : Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "#https://huggingface.co/gaussalgo/T5-LM-Large-text2sql-spider\n",
    "\n",
    "\n",
    "model_path = 'gaussalgo/T5-LM-Large-text2sql-spider'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_2_sql(input_text):\n",
    "    \"\"\" This function takes a natural language question and a schema as input and returns the SQL query\"\"\"\n",
    "    #Encode the input text\n",
    "    model_inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    #Invoke the model\n",
    "    outputs = model.generate(**model_inputs, max_length=512)\n",
    "    #Decode the model outputs\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT count(*) FROM inventory']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_sql(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT sum(T1.count) FROM inventory AS T1 JOIN product AS T2 ON T1.product = T2.product WHERE T2.product_name = 'apple'\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_sql(\"Translate the following text to SQL: Show me the total number of rows from the inventory table where the product name is 'apple'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use a pipeline as a high-level helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"gaussalgo/T5-LM-Large-text2sql-spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SELECT count(*) FROM inventory'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Pipeline (langchain_huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sin prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM inventory'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Example using from_model_id\n",
    "hf_pipeline = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gaussalgo/T5-LM-Large-text2sql-spider\",\n",
    "    task=\"text2text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 500},\n",
    ")\n",
    "\n",
    "hf_pipeline.invoke(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "### System:\n",
    "You are a SQL expert. Given an input question, your job is to translate the text to SQL.\n",
    "### User:\n",
    "{question}\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT sum(*) FROM inventory'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain with a custom prompt and the HF pipeline\n",
    "chain = prompt | hf_pipeline\n",
    "\n",
    "question = \"Show me the total number of rows from the inventory\"\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Inference API  (langchain_huggingface)\n",
    "\n",
    "\n",
    "HuggingFaceEndpoint integration of the free Serverless Endpoints API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "_ = load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sin prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT DISTINCT stock FROM inventory'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "client = InferenceClient(\n",
    "    model=\"gaussalgo/T5-LM-Large-text2sql-spider\", \n",
    "    token=os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Define the text input for SQL generation\n",
    "input_text = \"Translate this to SQL: Show me the stock of all items.\"\n",
    "\n",
    "# Call the model using `text_generation`\n",
    "response = client.text_generation(input_text,)\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT stock FROM inventory'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Inicializar el cliente de Hugging Face\n",
    "client = InferenceClient(\n",
    "    model=\"gaussalgo/T5-LM-Large-text2sql-spider\", \n",
    "    token=os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Prompt personalizado para mejorar la precisión\n",
    "custom_prompt = \"\"\"\n",
    "You are an AI specialized in translating natural language queries into SQL queries.\n",
    "The database follows standard SQL syntax.\n",
    "Do not include explanations, only return the SQL query.\n",
    "\n",
    "Translate this request into SQL:\n",
    "\"\"\"\n",
    "\n",
    "# Consulta en lenguaje natural\n",
    "user_query = \"Show me the stock of all items.\"\n",
    "\n",
    "# Generar la entrada final para el modelo\n",
    "input_text = f\"{custom_prompt}\\n{user_query}\"\n",
    "\n",
    "# Llamar al modelo para generar la consulta SQL\n",
    "response = client.text_generation(input_text)\n",
    "\n",
    "response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
