{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e77c0c2b23548d7a3097d54683063b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InferenceClient aims to provide a unified experience to perform inference. The client can be used seamlessly with either the (free) Inference API, self-hosted Inference Endpoints, or third-party Inference Providers. https://huggingface.co/docs/huggingface_hub/v0.28.1/en/package_reference/inference_client#huggingface_hub.InferenceClient.chat_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modelo de meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None), logprobs=None)], created=1739358178, id='', model='meta-llama/Meta-Llama-3-8B-Instruct', system_fingerprint='3.0.1-sha-bb9095a', usage=ChatCompletionOutputUsage(completion_tokens=8, prompt_tokens=17, total_tokens=25))\n",
      "response: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "response = client.chat_completion(messages, max_tokens=100)\n",
    "\n",
    "print(response)\n",
    "print(\"response:\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modelo de Qwen2.5-Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "qwen_client = InferenceClient(\"Qwen/Qwen2.5-Coder-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='To show the total number of rows from the inventory table, you can use the following SQL query:\\n\\n```sql\\nSELECT COUNT(*) AS total_rows\\nFROM inventory;\\n```\\n\\nThis query will return a result set with one column and one row, where the value of the `total_rows` column will be the total number of rows in the inventory table.', tool_calls=None), logprobs=None)], created=1739358587, id='', model='Qwen/Qwen2.5-Coder-1.5B-Instruct', system_fingerprint='3.0.1-sha-bb9095a', usage=ChatCompletionOutputUsage(completion_tokens=72, prompt_tokens=36, total_tokens=108))\n",
      "response: To show the total number of rows from the inventory table, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_rows\n",
      "FROM inventory;\n",
      "```\n",
      "\n",
      "This query will return a result set with one column and one row, where the value of the `total_rows` column will be the total number of rows in the inventory table.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are expert in SQL and can translate natural language to SQL\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show me the total number of rows from the inventory table\"}\n",
    "]\n",
    "\n",
    "response = qwen_client.chat_completion(messages)\n",
    "\n",
    "print(response)\n",
    "print(\"response:\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modelo de T5-LM-Large-text2sql-spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "spider_client = InferenceClient(\"gaussalgo/T5-LM-Large-text2sql-spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are expert in SQL and can translate natural language to SQL\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show me the total number of rows from the inventory table\"}\n",
    "]\n",
    "\n",
    "response = spider_client.chat_completion(messages)\n",
    "\n",
    "print(response)\n",
    "print(\"response:\",response.choices[0].message.content)\n",
    "# 400 Client Error: Bad Request for url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I am not capable of experiencing emotions or sensations like humans do. However, I am programmed to respond in a polite and friendly manner. So, I'm doing well, thank you for asking. How about you?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=47, prompt_tokens=28, total_tokens=75), 'model': '', 'finish_reason': 'stop'}, id='run-b4714fb5-b95c-4e7e-93e1-9d4ec7f95b81-0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "I hope this email finds you amidst an aura of understanding, despite the tangled mess of emotions swirling within me as I write to you. I am writing to pour my heart out about the recent unfortunate experience I had with one of your coffee machines that arrived ominously broken, evoking a profound sense of disbelief and despair.\n",
    "\n",
    "To set the scene, let me paint you a picture of the moment I anxiously unwrapped the box containing my highly anticipated coffee machine. The blatant excitement coursing through my veins could rival the vigorous flow of coffee through its finest espresso artistry. However, what I discovered within broke not only my spirit but also any semblance of confidence I had placed in your esteemed brand.\n",
    "\n",
    "Imagine, if you can, the utter shock and disbelief that took hold of me as I laid eyes on a disheveled and mangled coffee machine. Its once elegant exterior was marred by the scars of travel, resembling a war-torn soldier who had fought valiantly on the fields of some espresso battlefield. This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection, leaving me emotionally distraught and inconsolable\n",
    "\"\"\"  # created by GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The writer is sharing their disappointment about a coffee machine they received that arrived broken, despite their high expectations. They describe feeling a mix of disbelief, despair, and emotional distress upon discovering the damaged machine, as its appearance reminded them of a damaged soldier. The writer's initial excitement about using the machine was shattered, leaving them feeling upset and disappointed.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=75, prompt_tokens=315, total_tokens=390), 'model': '', 'finish_reason': 'stop'}, id='run-ed5870c1-0027-476c-a74f-486323a1c17d-0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=f\"Summarize this: {customer_email}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "chat_model.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option : Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "#https://huggingface.co/gaussalgo/T5-LM-Large-text2sql-spider\n",
    "\n",
    "\n",
    "model_path = 'gaussalgo/T5-LM-Large-text2sql-spider'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_2_sql(input_text):\n",
    "    \"\"\" This function takes a natural language question and a schema as input and returns the SQL query\"\"\"\n",
    "    #Encode the input text\n",
    "    model_inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    #Invoke the model\n",
    "    outputs = model.generate(**model_inputs, max_length=512)\n",
    "    #Decode the model outputs\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT count(*) FROM inventory']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_sql(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT sum(T1.count) FROM inventory AS T1 JOIN product AS T2 ON T1.product = T2.product WHERE T2.product_name = 'apple'\"]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_sql(\"Translate the following text to SQL: Show me the total number of rows from the inventory table where the product name is 'apple'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use a pipeline as a high-level helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"gaussalgo/T5-LM-Large-text2sql-spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SELECT count(*) FROM inventory'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Pipeline (langchain_huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sin prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM inventory'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Example using from_model_id\n",
    "hf_pipeline = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gaussalgo/T5-LM-Large-text2sql-spider\",\n",
    "    task=\"text2text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 500},\n",
    ")\n",
    "\n",
    "hf_pipeline.invoke(\"Translate the following text to SQL: Show me the total number of rows from the inventory table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "### System:\n",
    "You are a SQL expert. Given an input question, your job is to translate the text to SQL.\n",
    "### User:\n",
    "{question}\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT sum(*) FROM inventory'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain with a custom prompt and the HF pipeline\n",
    "chain = prompt | hf_pipeline\n",
    "\n",
    "question = \"Show me the total number of rows from the inventory\"\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Inference API  (langchain_huggingface)\n",
    "\n",
    "\n",
    "HuggingFaceEndpoint integration of the free Serverless Endpoints API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "_ = load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sin prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT DISTINCT stock FROM inventory'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "client = InferenceClient(\n",
    "    model=\"gaussalgo/T5-LM-Large-text2sql-spider\", \n",
    "    token=os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Define the text input for SQL generation\n",
    "input_text = \"Translate this to SQL: Show me the stock of all items.\"\n",
    "\n",
    "# Call the model using `text_generation`\n",
    "response = client.text_generation(input_text,)\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT stock FROM inventory'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Inicializar el cliente de Hugging Face\n",
    "client = InferenceClient(\n",
    "    model=\"gaussalgo/T5-LM-Large-text2sql-spider\", \n",
    "    token=os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Prompt personalizado para mejorar la precisiÃ³n\n",
    "custom_prompt = \"\"\"\n",
    "You are an AI specialized in translating natural language queries into SQL queries.\n",
    "The database follows standard SQL syntax.\n",
    "Do not include explanations, only return the SQL query.\n",
    "\n",
    "Translate this request into SQL:\n",
    "\"\"\"\n",
    "\n",
    "# Consulta en lenguaje natural\n",
    "user_query = \"Show me the stock of all items.\"\n",
    "\n",
    "# Generar la entrada final para el modelo\n",
    "input_text = f\"{custom_prompt}\\n{user_query}\"\n",
    "\n",
    "# Llamar al modelo para generar la consulta SQL\n",
    "response = client.text_generation(input_text)\n",
    "\n",
    "response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
